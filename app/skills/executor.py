from __future__ import annotations

import asyncio
import logging
from collections.abc import Awaitable, Callable
from pathlib import Path
from typing import TYPE_CHECKING

from app.formatting.compaction import compact_tool_output
from app.llm.client import OllamaClient
from app.models import ChatMessage
from app.security.audit import AuditTrail
from app.security.policy_engine import PolicyEngine
from app.skills.models import ToolCall
from app.skills.registry import SkillRegistry
from app.skills.router import classify_intent, select_tools
from app.tracing.context import get_current_trace

if TYPE_CHECKING:
    from app.mcp.manager import McpManager

logger = logging.getLogger(__name__)

MAX_TOOL_ITERATIONS = 5

# Module-level cache: tools don't change at runtime after initialization
_cached_tools_map: dict[str, dict] | None = None

_policy_engine: PolicyEngine | None = None
_audit_trail: AuditTrail | None = None


async def get_policy_engine() -> PolicyEngine:
    global _policy_engine
    if _policy_engine is None:
        loop = asyncio.get_running_loop()
        _policy_engine = await loop.run_in_executor(None, lambda: PolicyEngine(Path("data/security_policies.yaml")))
    return _policy_engine


async def get_audit_trail() -> AuditTrail:
    global _audit_trail
    if _audit_trail is None:
        loop = asyncio.get_running_loop()
        _audit_trail = await loop.run_in_executor(None, lambda: AuditTrail(Path("data/audit_trail.jsonl")))
    return _audit_trail


def _build_tools_map(
    skill_registry: SkillRegistry,
    mcp_manager: McpManager | None,
) -> dict[str, dict]:
    """Build a name -> ollama schema dict from all available tools."""
    tools_map: dict[str, dict] = {}
    for tool_schema in skill_registry.get_ollama_tools():
        name = tool_schema["function"]["name"]
        tools_map[name] = tool_schema
    if mcp_manager:
        for tool_schema in mcp_manager.get_ollama_tools():
            name = tool_schema["function"]["name"]
            tools_map[name] = tool_schema
    return tools_map


def _get_cached_tools_map(
    skill_registry: SkillRegistry,
    mcp_manager: McpManager | None,
) -> dict[str, dict]:
    """Return cached tools map, building it on first call."""
    global _cached_tools_map
    if _cached_tools_map is None:
        _cached_tools_map = _build_tools_map(skill_registry, mcp_manager)
    return _cached_tools_map


def reset_tools_cache() -> None:
    """Invalidate the cached tools map so it rebuilds on the next request.

    Call this after hot-adding or hot-removing MCP servers, or after
    reloading skills, to ensure the executor picks up the new tools.
    """
    global _cached_tools_map
    _cached_tools_map = None
    logger.info("Tools cache invalidated")


async def _run_tool_call(
    tc: dict,
    skill_registry: SkillRegistry,
    mcp_manager: McpManager | None,
    ollama_client: OllamaClient,
    user_message: str,
    hitl_callback: Callable[[str, dict, str], Awaitable[bool]] | None = None,
) -> ChatMessage:
    """Execute a single tool call and return the result as a tool message."""
    func = tc.get("function", {})
    tool_name = func.get("name", "")
    arguments = func.get("arguments", {})

    # Lazy-load skill instructions on first use (only for local skills)
    instructions = skill_registry.get_skill_instructions(tool_name)

    policy = await get_policy_engine()
    audit = await get_audit_trail()

    loop = asyncio.get_running_loop()

    decision = policy.evaluate(tool_name, arguments)

    if decision.is_blocked:
        await loop.run_in_executor(
            None, lambda: audit.record(tool_name, arguments, "block", decision.reason, "blocked_by_policy")
        )
        error_msg = f"Security Policy Blocked execution: {decision.reason}"
        logger.warning(f"Blocked tool {tool_name}: {decision.reason}")
        return ChatMessage(role="tool", content=error_msg)

    if decision.requires_flag:
        if hitl_callback:
            await loop.run_in_executor(
                None, lambda: audit.record(tool_name, arguments, "flag", decision.reason, "pending_hitl_approval")
            )
            logger.warning(f"Tool {tool_name} flagged for HITL approval: {decision.reason}")
            try:
                approved = await hitl_callback(tool_name, arguments, decision.reason or "")
                if not approved:
                    await loop.run_in_executor(
                        None, lambda: audit.record(
                            tool_name, arguments, "blocked_via_hitl", decision.reason, "denied_by_user"
                        )
                    )
                    return ChatMessage(
                        role="tool", content="Security Policy BLOCK: Execution denied by user."
                    )
                await loop.run_in_executor(
                    None, lambda: audit.record(
                        tool_name, arguments, "allowed_via_hitl", decision.reason, "approved_by_user"
                    )
                )
            except Exception as e:
                return ChatMessage(role="tool", content=f"HITL error: {e}")
        else:
            await loop.run_in_executor(
                None, lambda: audit.record(tool_name, arguments, "block", decision.reason, "no_hitl_available")
            )
            return ChatMessage(
                role="tool", content="Security Policy BLOCK: Flagged but no HITL provided."
            )

    tool_call = ToolCall(name=tool_name, arguments=arguments)

    trace = get_current_trace()
    if trace:
        async with trace.span(f"tool:{tool_name}", kind="tool") as span:
            span.set_input({"tool": tool_name, "arguments": arguments})
            if mcp_manager and mcp_manager.has_tool(tool_name):
                result = await mcp_manager.execute_tool(tool_call)
            else:
                result = await skill_registry.execute_tool(tool_call)
            span.set_output({"content": result.content[:200]})
    else:
        if mcp_manager and mcp_manager.has_tool(tool_name):
            result = await mcp_manager.execute_tool(tool_call)
        else:
            result = await skill_registry.execute_tool(tool_call)

    # Runtime fallback: if a Puppeteer tool fails, retry with mcp-fetch (plain HTTP)
    if (
        not result.success
        and tool_name.startswith("puppeteer_")
        and mcp_manager is not None
    ):
        mcp_fetch_tools = {
            name
            for name, tool in mcp_manager.get_tools().items()
            if tool.skill_name == "mcp::mcp-fetch"
        }
        if mcp_fetch_tools:
            url = arguments.get("url") or arguments.get("name") or arguments.get("input", "")
            if url:
                fallback_name = next(
                    (t for t in ("fetch_markdown", "fetch", "fetch_txt") if t in mcp_fetch_tools),
                    None,
                )
                if fallback_name:
                    logger.warning(
                        "Puppeteer tool %s failed, retrying with mcp-fetch fallback (%s)",
                        tool_name,
                        fallback_name,
                    )
                    from app.skills.models import ToolCall as _ToolCall

                    fallback_call = _ToolCall(name=fallback_name, arguments={"url": url})
                    fallback_result = await mcp_manager.execute_tool(fallback_call)
                    prefix = "[⚠️ Fallback a mcp-fetch — Puppeteer no respondió]\n"
                    from app.skills.models import ToolResult as _ToolResult

                    result = _ToolResult(
                        tool_name=fallback_name,
                        content=prefix + fallback_result.content,
                        success=fallback_result.success,
                    )

    # Record allowed execution in audit log
    await loop.run_in_executor(
        None, lambda: audit.record(tool_name, arguments, "allow", decision.reason, result.content[:200])
    )

    logger.debug("Tool Execution RAW PAYLOAD send to %s: %s", tool_name, arguments)
    logger.debug("Tool Execution RAW OUPUT from %s: %r", tool_name, result.content)
    logger.info("Tool %s -> %s", tool_name, result.content[:100])

    final_content = result.content

    # Compress the context if it is massive
    final_content = await compact_tool_output(
        tool_name=tool_name,
        text=final_content,
        user_request=user_message,
        ollama_client=ollama_client,
    )

    if instructions:
        final_content = f"{instructions}\n\nResult:\n{final_content}"

    return ChatMessage(role="tool", content=final_content)


async def execute_tool_loop(
    messages: list[ChatMessage],
    ollama_client: OllamaClient,
    skill_registry: SkillRegistry,
    mcp_manager: McpManager | None = None,
    max_tools: int = 8,
    pre_classified_categories: list[str] | None = None,
    user_facts: dict[str, str] | None = None,
    recent_messages: list[ChatMessage] | None = None,
    sticky_categories: list[str] | None = None,
    hitl_callback: Callable[[str, dict, str], Awaitable[bool]] | None = None,
) -> str:
    """Run the tool calling loop: classify intent, select tools, execute.

    Args:
        messages: Conversation history including the latest user message.
        ollama_client: LLM client.
        skill_registry: Registry of available tools.
        mcp_manager: Optional MCP manager for external tools.
        max_tools: Maximum number of tools to offer the LLM per iteration.
        pre_classified_categories: Skip classification if already done.
        user_facts: Structured user facts from memory (e.g. github_username). Injected
            as a system message so the LLM has explicit access during tool calls.
        recent_messages: Recent conversation history for contextual classification.
        sticky_categories: Fallback categories from previous tool-using turn.
    """
    # Always extract last user message — needed for tool output compaction
    # regardless of whether intent classification is pre-computed.
    user_message = ""
    for msg in reversed(messages):
        if msg.role == "user":
            user_message = msg.content
            break

    # Stage 1: classify intent (use pre-computed result if available)
    all_tools_map = _get_cached_tools_map(skill_registry, mcp_manager)
    categories = pre_classified_categories or await classify_intent(
        user_message,
        ollama_client,
        recent_messages=recent_messages,
        sticky_categories=sticky_categories,
    )

    if categories == ["none"]:
        logger.info("Tool router: categories=none, plain chat")
        return await ollama_client.chat(messages)

    # Stage 2: select relevant tools
    tools = select_tools(categories, all_tools_map, max_tools=max_tools)
    logger.info(
        "Tool router: categories=%s, selected %d tools: %s",
        categories,
        len(tools),
        [t.get("function", {}).get("name") for t in tools],
    )

    if not tools:
        logger.info("Tool router: no matching tools found, plain chat")
        return await ollama_client.chat(messages)

    working_messages = list(messages)

    # Inject user facts as a system message so the LLM has explicit access during tool calls.
    # This prevents the LLM from guessing or hallucinating values like owner names in GitHub calls.
    if user_facts:
        from app.context.fact_extractor import format_facts_for_prompt

        facts_text = format_facts_for_prompt(user_facts)
        if facts_text:
            working_messages.insert(
                1 if working_messages and working_messages[0].role == "system" else 0,
                ChatMessage(role="system", content=facts_text),
            )
            logger.debug("Injected user_facts into tool loop: %s", list(user_facts.keys()))

    for iteration in range(MAX_TOOL_ITERATIONS):
        response = await ollama_client.chat_with_tools(working_messages, tools=tools)

        if not response.tool_calls:
            logger.info(
                "Tool iteration %d: LLM decided to reply directly: %r",
                iteration + 1,
                response.content[:150],
            )
            return response.content

        tool_names = [tc.get("function", {}).get("name") for tc in response.tool_calls]
        logger.info(
            "Tool iteration %d: LLM generated %d tool call(s): %s",
            iteration + 1,
            len(response.tool_calls),
            tool_names,
        )

        # Append assistant message with tool_calls
        working_messages.append(
            ChatMessage(
                role="assistant",
                content=response.content or "",
                tool_calls=response.tool_calls,
            )
        )

        # Execute all tool calls in parallel, append results in order
        tool_messages = await asyncio.gather(
            *[
                _run_tool_call(
                    tc, skill_registry, mcp_manager, ollama_client, user_message, hitl_callback
                )
                for tc in response.tool_calls
            ]
        )
        working_messages.extend(tool_messages)

        # Tool result clearing: replace old (raw) tool results with compact placeholders
        # to prevent context bloat on iterations 3+. Keep the last 2 rounds intact.
        _clear_old_tool_results(working_messages, keep_last_n=2)

    # Safety: exceeded max iterations, force a text response without tools
    logger.warning("Max tool iterations (%d) reached, forcing text response", MAX_TOOL_ITERATIONS)
    response = await ollama_client.chat_with_tools(working_messages, tools=None)
    return response.content


def _clear_old_tool_results(messages: list[ChatMessage], keep_last_n: int = 2) -> None:
    """Replace old tool results with compact summaries to free context window space.

    Keeps the last `keep_last_n` tool messages intact (most recent are most useful).
    This implements the Anthropic-recommended 'tool result clearing' pattern:
    once a raw API response is processed, there's no reason to keep it verbatim.
    """
    tool_indices = [i for i, m in enumerate(messages) if m.role == "tool"]

    if len(tool_indices) <= keep_last_n:
        return  # Not enough tool results to warrant clearing

    for idx in tool_indices[:-keep_last_n]:
        old_content = messages[idx].content
        first_line = old_content.split("\n")[0][:120].strip()
        messages[idx] = ChatMessage(
            role="tool",
            content=f"[Previous result processed — summary: {first_line}]",
        )

# WasAP - Plan de Producto
## Asistente Personal vÃ­a WhatsApp con LLM Local (Ollama)

---

## 1. VisiÃ³n del Producto

Un asistente personal inteligente al que le hablÃ¡s por WhatsApp y te responde usando un LLM corriendo en tu mÃ¡quina via Ollama. Conversaciones contextuales, memoria persistente, y cero costo de operaciÃ³n.

**Principios:**
- **Privacidad**: el LLM y toda la lÃ³gica corren local. Los mensajes pasan por Meta (inevitable con WhatsApp) pero el procesamiento es 100% tuyo
- **Costo cero**: WhatsApp Cloud API gratis para mensajes de servicio + Ollama local + ngrok free
- **Simplicidad**: se levanta con `docker compose up`, no requiere infra cloud
- **Conversacional**: mantiene contexto y memoria entre sesiones
- **Extensible**: sistema de skills/plugins inspirado en [OpenClaw](https://openclaw.ai), donde agregar capacidades nuevas es escribir un archivo markdown

**InspiraciÃ³n â€” OpenClaw:**
WasAP toma varios patrones de diseÃ±o de OpenClaw, el asistente personal open-source que usa markdown como capa de configuraciÃ³n y memoria. En particular:
- **Archivos markdown como fuente de verdad** para memorias y configuraciÃ³n de skills
- **Skills como carpetas con SKILL.md** que definen capacidades del agente
- **Memoria en dos capas**: hechos curados (MEMORY.md) + notas diarias (logs)
- **Carga progresiva**: solo se lee el detalle de un skill cuando el agente lo necesita

La diferencia clave: OpenClaw es un framework multi-canal multi-agente. WasAP es un asistente personal single-user, single-channel (WhatsApp), optimizado para simplicidad.

---

## 2. Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    internet     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tu celular  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Meta / WhatsApp â”‚
â”‚  (WhatsApp)  â”‚                 â”‚  Cloud API       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚ webhook HTTPS
                                          â–¼
                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚  ngrok tunnel    â”‚
                                 â”‚  (free tier)     â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚ localhost:8000
                                          â–¼
                                 â”Œâ”€â”€â”€ Docker Compose â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                 â”‚                                      â”‚
                                 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
                                 â”‚  â”‚  WasAP Server (Python)   â”‚        â”‚
                                 â”‚  â”‚                         â”‚        â”‚
                                 â”‚  â”‚  FastAPI                â”‚        â”‚
                                 â”‚  â”‚  â”œâ”€ Webhook receiver    â”‚        â”‚
                                 â”‚  â”‚  â”œâ”€ Command router      â”‚        â”‚
                                 â”‚  â”‚  â”œâ”€ Skill engine        â”‚        â”‚
                                 â”‚  â”‚  â”œâ”€ Conversation mgr    â”‚        â”‚
                                 â”‚  â”‚  â”œâ”€ Memory manager      â”‚        â”‚
                                 â”‚  â”‚  â””â”€ WhatsApp sender     â”‚        â”‚
                                 â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                                 â”‚      â”‚         â”‚                    â”‚
                                 â”‚ â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â” â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”            â”‚
                                 â”‚ â”‚ Ollama â”‚ â”‚  SQLite  â”‚            â”‚
                                 â”‚ â”‚ (LLM)  â”‚ â”‚ (volume) â”‚            â”‚
                                 â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de un mensaje

1. EscribÃ­s un mensaje en WhatsApp
2. Meta lo envÃ­a como webhook POST a tu dominio ngrok
3. ngrok lo tuneliza a tu `localhost:8000`
4. FastAPI recibe el webhook, valida la firma, extrae el mensaje
5. Si es un `/comando` â†’ se ejecuta directamente sin pasar por el LLM
6. Si es texto normal:
   - Se guarda en SQLite
   - Se cargan memorias activas + resumen previo + historial reciente
   - Se arma el contexto (system prompt + memorias + summary + historial)
   - Se envÃ­a a Ollama API local
   - La respuesta se guarda y se envÃ­a por WhatsApp
   - Si el historial supera el threshold, se lanza un resumen en background

---

## 3. Stack TecnolÃ³gico

| Componente | TecnologÃ­a | Por quÃ© |
|---|---|---|
| Servidor | **Python 3.11+ / FastAPI** | Async, rÃ¡pido, ideal para webhooks |
| WhatsApp | **WhatsApp Cloud API** (oficial) | Gratis para servicio, sin riesgo de ban |
| TÃºnel | **ngrok** (free tier) | Dominio estÃ¡tico gratis, sin timeout, 1GB/mes sobra |
| LLM | **Ollama** | Local, gratuito, mÃºltiples modelos, tool calling |
| Base de datos | **SQLite** (WAL mode) + **aiosqlite** | Sin servidor, async, suficiente para 1 usuario |
| Config | **archivo .env** + **markdown** | Simple, estÃ¡ndar, human-readable |
| Contenedores | **Docker + Docker Compose** | Setup reproducible, un solo comando para levantar todo |

### Por quÃ© NO Baileys
- Riesgo creciente de ban permanente en 2026
- Viola ToS de WhatsApp
- Meta estÃ¡ detectando activamente clientes no oficiales
- Para un asistente personal de largo plazo, la estabilidad es clave

### Por quÃ© WhatsApp Cloud API + ngrok
- **Oficial**: cero riesgo de ban
- **Gratis**: mensajes de servicio (respuesta dentro de 24h) no tienen costo
- **ngrok free**: dominio estÃ¡tico, sin timeout, 20k requests/mes
- **Trade-off aceptable**: el tÃºnel pasa por ngrok pero el procesamiento es local

---

## 4. Funcionalidades por Fase

### Fase 1: MVP - Chat funcional âœ…
> Poder mandar un mensaje por WhatsApp y recibir respuesta del LLM local.

- Webhook receiver con validaciÃ³n de firma de Meta
- EnvÃ­o de respuestas via Cloud API
- ConexiÃ³n a Ollama API (`/api/chat`)
- System prompt configurable
- Historial de conversaciÃ³n en memoria (Ãºltimos N mensajes)
- Whitelist de nÃºmeros (solo responde a tu nÃºmero)
- Indicador de "visto" / acuse de recibo
- Health check endpoint
- GuÃ­a de setup completa (SETUP.md)
- **Dockerizado**: Dockerfile + docker-compose.yml (wasap + ollama + ngrok)
- Volume para modelos de Ollama (persistencia entre reinicios)
- Tests unitarios completos

### Fase 2: Persistencia y Memoria âœ…
> El asistente recuerda conversaciones anteriores y datos que le pedÃ­s guardar.

- Persistencia en SQLite (WAL mode, aiosqlite)
- Cargar Ãºltimos N mensajes como contexto al recibir mensaje nuevo
- Resumen automÃ¡tico de conversaciones largas en background
- Sistema de memorias globales (SQLite source of truth + `data/MEMORY.md` mirror)
- Sistema de comandos extensible (registry pattern):
  - `/remember <dato>` â€” guardar informaciÃ³n importante
  - `/forget <dato>` â€” olvidar un recuerdo guardado
  - `/memories` â€” listar recuerdos guardados
  - `/clear` â€” borrar historial de conversaciÃ³n
  - `/help` â€” mostrar comandos disponibles
- Memorias inyectadas automÃ¡ticamente en el contexto del LLM
- Mensajes de error claros para token expirado, permisos faltantes, modelo no descargado
- Tests: 70+ tests cubriendo repository, commands, summarizer, memory mirror

### Fase 3: UX y Multimedia âœ…
> El asistente se siente mÃ¡s natural y maneja mÃ¡s que texto.

- **Audio entrante**: descarga de WhatsApp â†’ transcripciÃ³n con faster-whisper local (async via run_in_executor)
- **ImÃ¡genes entrantes**: descarga â†’ descripciÃ³n con llava:7b â†’ respuesta contextual con qwen3:8b
- **Formato WhatsApp**: conversiÃ³n markdownâ†’WhatsApp (*bold*, _italic_, ```code```, listas)
- **Mensajes largos**: split automÃ¡tico (>4096 chars) en mÃºltiples mensajes
- **Indicador de typing**: emoji â³ como reacciÃ³n durante procesamiento, removido en finally
- **Rate limiting**: in-memory, por nÃºmero de telÃ©fono
- **Logging estructurado**: JSON con python-json-logger

### Fase 4: Skills y Herramientas âœ…
> El asistente puede hacer cosas, no solo conversar.

Inspirado en el [sistema de skills de OpenClaw](https://docs.openclaw.ai/tools/skills), donde cada skill es una carpeta con un archivo markdown que define quÃ© puede hacer el agente.

#### Arquitectura de Skills

```
skills/
â”œâ”€â”€ datetime/
â”‚   â””â”€â”€ SKILL.md
â”œâ”€â”€ calculator/
â”‚   â””â”€â”€ SKILL.md
â”œâ”€â”€ weather/
â”‚   â””â”€â”€ SKILL.md
â””â”€â”€ notes/
    â””â”€â”€ SKILL.md
```

Cada `SKILL.md` tiene frontmatter YAML (parseado con regex, sin PyYAML) + instrucciones en prosa:

```yaml
---
name: weather
description: Consultar el clima actual y pronÃ³stico
version: 1
tools:
  - get_weather
---

Cuando el usuario pregunte por el clima, usÃ¡ la tool `get_weather` con la ciudad.
Si no dice ciudad, preguntale cuÃ¡l.
RespondÃ© en el idioma del usuario.
```

#### Carga progresiva (patrÃ³n OpenClaw)

1. **Al iniciar**: se lee solo `name` y `description` de cada SKILL.md (~30 tokens por skill)
2. **Al usar una tool**: se carga el cuerpo del SKILL.md correspondiente (lazy, una vez por skill)
3. **EjecuciÃ³n**: tool calling loop con max 5 iteraciones como safety cap

#### Tool calling via Ollama

`think: True` es incompatible con tools en qwen3. Cuando hay tools en el payload, se desactiva automÃ¡ticamente. El flujo:

1. El mensaje del usuario llega con la lista de tools disponibles
2. Ollama responde con tool_calls en vez de texto
3. WasAP ejecuta cada tool, appendea resultados como role="tool"
4. Se reenvÃ­a a Ollama â†’ puede llamar mÃ¡s tools o responder con texto
5. DespuÃ©s de MAX_TOOL_ITERATIONS (5), se fuerza respuesta sin tools

#### Skills implementados

| Skill | Tools | Mecanismo |
|-------|-------|-----------|
| `datetime` | `get_current_datetime`, `convert_timezone` | zoneinfo stdlib |
| `calculator` | `calculate` | AST safe eval (whitelist estricta, NO eval()) |
| `weather` | `get_weather` | wttr.in API (gratis, sin API key) |
| `notes` | `save_note`, `list_notes`, `search_notes`, `delete_note` | SQLite |

#### Reliability (incluido en Fase 4)

- **Dedup atÃ³mico**: tabla `processed_messages` con INSERT OR IGNORE (sin race conditions entre webhooks concurrentes)
- **Reply context**: si el usuario responde a un mensaje especÃ­fico, se inyecta `[Replying to: "..."]` en el prompt
- **Graceful shutdown**: tracking de background tasks, wait con timeout de 30s antes de cerrar DB/HTTP

#### Extensibilidad

- Agregar un skill = crear carpeta con SKILL.md + registrar handlers en Python
- El directorio de skills es configurable via `SKILLS_DIR` (default: `skills/`)
- Sin skills disponibles, el sistema se comporta exactamente como antes (backward compatible)

### Fase 5: Memoria Avanzada âœ…
> Sistema de memoria en capas con triggers automÃ¡ticos, inspirado en [OpenClaw](https://docs.openclaw.ai/concepts/memory) y Context Engineering (Google, Nov 2025).

#### TaxonomÃ­a de memoria (3 capas)

| Tipo | QuÃ© guarda | DÃ³nde | Ejemplo |
|------|-----------|-------|---------|
| **SemÃ¡ntica** | Hechos estables, preferencias | `data/MEMORY.md` + tabla `memories` | "El usuario prefiere respuestas en espaÃ±ol" |
| **EpisÃ³dica Reciente** | Contexto temporal (del dÃ­a) | `data/memory/YYYY-MM-DD.md` | "Hoy discutimos migrar la DB a Postgres" |
| **EpisÃ³dica HistÃ³rica** | Conversaciones pasadas (crudo) | `data/memory/snapshots/*.md` | Ãšltimos 15 msgs antes de /clear |

#### 5A: Daily Logs + Bootstrap Loading

- **Daily logs**: archivos append-only `data/memory/YYYY-MM-DD.md` con entries timestamped
- **Bootstrap loading**: `ConversationManager.get_context()` inyecta daily logs (hoy + ayer) como system message
- Los daily logs se escriben durante el flush (5B) y snapshots (5C), no en cada mensaje
- Configurable: `memory_dir`, `daily_log_days` en Settings

#### 5B: Pre-Compaction Flush (Write-Ahead Log)

- Antes de que el summarizer borre mensajes, `flush_to_memory()` los analiza con el LLM
- Extrae **facts** â†’ `repository.add_memory()` + sync MEMORY.md
- Extrae **events** â†’ `daily_log.append()`
- **Dedup**: `difflib.SequenceMatcher(ratio > 0.8)` contra memorias existentes
- JSON output format, con fallback para code-fenced responses
- Configurable: `memory_flush_enabled` en Settings

#### 5C: Session Snapshots

- **Trigger**: comando `/clear`
- Antes de borrar, guarda los Ãºltimos 15 mensajes (user + assistant) como snapshot
- Slug descriptivo generado por LLM (`think=False`), fallback a timestamp
- Snapshot guardado en `data/memory/snapshots/YYYY-MM-DD-slug.md`
- Entry en daily log: "Session cleared: topic (N messages saved)"

#### 5D: Memory Consolidation

- DespuÃ©s del flush, si se agregaron memorias nuevas, el LLM revisa todas las memorias
- Identifica duplicados y contradicciones â†’ soft-delete los redundantes
- MÃ­nimo 8 memorias para activar consolidaciÃ³n
- Solo se ejecuta cuando el flush agrega â‰¥1 memoria nueva

### Fase 6: BÃºsqueda SemÃ¡ntica y RAG âœ…
> Embeddings locales, bÃºsqueda semÃ¡ntica y MEMORY.md bidireccional.

#### BÃºsqueda semÃ¡ntica

- **Embeddings locales** via Ollama (`nomic-embed-text`, 768 dims)
- **sqlite-vec** para KNN search nativo en SQLite (tablas virtuales `vec_memories`, `vec_notes`)
- Solo las memorias y notas **relevantes** al mensaje se inyectan en el contexto del LLM
- Query embedding se computa una sola vez y se reutiliza para memorias + notas
- Fallback graceful: si sqlite-vec no carga o embed() falla â†’ comportamiento anterior (todas las memorias)
- Master switch: `SEMANTIC_SEARCH_ENABLED` (default `true`)

#### Auto-indexing

- `/remember` â†’ embede la nueva memoria automÃ¡ticamente
- `/forget` â†’ borra el embedding
- Pre-compaction flush â†’ embede cada fact auto-extraÃ­do
- Consolidador â†’ borra embeddings de memorias removidas
- Startup â†’ backfill de todas las memorias y notas sin embedding

#### MEMORY.md bidireccional

- Editar MEMORY.md a mano â†’ watchdog (inotify en Linux) detecta el cambio â†’ sync a SQLite
- Sync guard (`threading.Event`) previene loops de retroalimentaciÃ³n
- Maneja `on_created` ademÃ¡s de `on_modified` para editores con atomic rename (vim, etc.)

#### Notas semÃ¡nticas

- `search_notes` usa bÃºsqueda semÃ¡ntica con fallback a LIKE
- `save_note` auto-embede; `delete_note` limpia el embedding
- Notas relevantes inyectadas como contexto del LLM (via `get_context(relevant_notes=...)`)

### Fase 7: Performance Optimization âœ…
> Optimizaciones quirÃºrgicas para reducir latencia sin cambios de arquitectura.

#### Problema
El critical path para un mensaje de texto tomaba 5-12s con todas las operaciones secuenciales:
- DB queries, embed, file I/O y `classify_intent` (LLM, 1-3s) todos en serie
- File I/O en `daily_log` y `markdown.sync()` bloqueando el event loop
- `get_or_create_conversation()` llamado 3-5 veces por mensaje para el mismo nÃºmero
- `_build_tools_map()` reconstruido en cada mensaje aunque nunca cambia en runtime
- Tool calls dentro de una iteraciÃ³n ejecutados secuencialmente

#### Optimizaciones implementadas

| Cambio | Archivo | Ganancia |
|--------|---------|---------|
| Critical path parallelizado en fases A/B/C | `webhook/router.py` | ~3-5s |
| `classify_intent` kickeado como task paralelo | `webhook/router.py` | solapado con I/O |
| WA calls iniciales en paralelo (mark_as_read + reaction) | `webhook/router.py` | ~200ms |
| `_build_context()` helper (sin DB calls) | `webhook/router.py` | cÃ³digo limpio |
| `pre_classified_categories` evita segundo classify | `skills/executor.py` | ~1-3s |
| Module-level cache de tools_map | `skills/executor.py` | ~5ms/msg |
| Tool calls paralelos por iteraciÃ³n | `skills/executor.py` | variable |
| Blocking I/O â†’ `asyncio.to_thread()` | `daily_log.py`, `markdown.py` | evita stall |
| Cache de conv_id en `ConversationManager` | `conversation/manager.py` | ~4 DB hits/msg |
| `get_active_memories(limit=...)` | `database/repository.py` | fallback limitado |
| SQLite PRAGMA tuning (synchronous, cache, temp) | `database/db.py` | I/O mÃ¡s rÃ¡pido |
| Model warmup en startup | `app/main.py` | cold-start eliminado |

#### Arquitectura del nuevo critical path
```
0. reply_context (solo si hay reply, sequential)
1. get_conversation_id (una sola DB hit, cacheable)
   + asyncio.create_task(classify_intent(...))  â† LLM call en background

Phase A (asyncio.gather):
   embed(user_text) â€– save_message(conv_id) â€– load_daily_logs()

Phase B (asyncio.gather):
   search_memories â€– search_notes â€– get_latest_summary â€– get_recent_messages

Phase C: categories = await classify_task  â† ya casi listo

Phase D: _build_context() â†’ chat_with_tools (LLM principal, ~3-8s)
```

### Fase 8: EvaluaciÃ³n y Mejora Continua ðŸ”„
> Arquitectura de evaluaciÃ³n estructurada para medir y mejorar sistemÃ¡ticamente la calidad del asistente.

- **Guardrails**: ValidaciÃ³n pre-entrega (idioma, longitud, alucinaciones, PII)
- **Trazabilidad Estructurada**: Spans jerÃ¡rquicos de todo el pipeline (SQLite)
- **EvaluaciÃ³n en 3 Capas**:
  - *Capa 1* (ImplÃ­cita): Reacciones de WhatsApp, correcciones del usuario
  - *Capa 2* (AutomÃ¡tica): G-Eval offline para testear mÃ©tricas
  - *Capa 3* (ExplÃ­cita): Comando `/feedback` para human-in-the-loop
- **Dataset vivo y Auto-evoluciÃ³n**: CreaciÃ³n de un dataset de interacciones y prompts dinÃ¡micos
- Ver detalle en `docs/exec-plans/eval_implementation_plan.md`

---

## 5. Modelo de Datos (SQLite)

### Actual (Fase 1-6)

```
conversations
â”œâ”€â”€ id            INTEGER PRIMARY KEY
â”œâ”€â”€ phone_number  TEXT UNIQUE
â”œâ”€â”€ created_at    TEXT
â””â”€â”€ updated_at    TEXT

messages
â”œâ”€â”€ id              INTEGER PRIMARY KEY
â”œâ”€â”€ conversation_id INTEGER FK
â”œâ”€â”€ role            TEXT (user/assistant/system)
â”œâ”€â”€ content         TEXT
â”œâ”€â”€ wa_message_id   TEXT UNIQUE
â””â”€â”€ created_at      TEXT

summaries
â”œâ”€â”€ id              INTEGER PRIMARY KEY
â”œâ”€â”€ conversation_id INTEGER FK
â”œâ”€â”€ content         TEXT
â”œâ”€â”€ message_count   INTEGER
â””â”€â”€ created_at      TEXT

memories
â”œâ”€â”€ id         INTEGER PRIMARY KEY
â”œâ”€â”€ content    TEXT
â”œâ”€â”€ category   TEXT (nullable)
â”œâ”€â”€ active     INTEGER (soft delete)
â””â”€â”€ created_at TEXT

notes
â”œâ”€â”€ id         INTEGER PRIMARY KEY
â”œâ”€â”€ title      TEXT
â”œâ”€â”€ content    TEXT
â””â”€â”€ created_at TEXT

processed_messages
â””â”€â”€ wa_message_id  TEXT PRIMARY KEY  (dedup atÃ³mico)
â””â”€â”€ processed_at   TEXT

vec_memories (sqlite-vec virtual table)
â”œâ”€â”€ memory_id  INTEGER PRIMARY KEY
â””â”€â”€ embedding  float[768]

vec_notes (sqlite-vec virtual table)
â”œâ”€â”€ note_id    INTEGER PRIMARY KEY
â””â”€â”€ embedding  float[768]
```

---

## 6. Modelos Recomendados (Ollama)

| Modelo | Params | RAM MÃ­n. | Caso de uso | EspaÃ±ol | Tools |
|---|---|---|---|---|---|
| `qwen3:8b` | 8B | 8GB | **Recomendado â€” chat + tools** | Excelente | SÃ­ |
| `llava:7b` | 7B | 8GB | Multimodal â€” imÃ¡genes | Limitado | No |
| `nomic-embed-text` | 137M | 1GB | Embeddings (768 dims) | SÃ­ | â€” |
| `llama3.2:8b` | 8B | 8GB | Alternativa chat | Bueno | SÃ­ |
| `llama3.2:3b` | 3B | 4GB | Hardware limitado | Aceptable | Limitado |

**RecomendaciÃ³n**: usar `qwen3:8b` para chat + tool calling, `llava:7b` para visiÃ³n, y `nomic-embed-text` para embeddings.

---

## 7. ConfiguraciÃ³n Necesaria (Meta Developer)

Para usar WhatsApp Cloud API necesitÃ¡s:

1. **Cuenta de Meta Developer** (gratis) â†’ developers.facebook.com
2. **Crear una App** tipo "Business"
3. **Agregar producto WhatsApp** a la app
4. **NÃºmero de prueba**: Meta te da un nÃºmero de test gratuito
5. **Webhook URL**: tu dominio ngrok estÃ¡tico
6. **Verify token**: string secreto que elegÃ­s vos
7. **Access token**: lo genera la plataforma (permanente con System User)

Todo gratis. El nÃºmero de test tiene limitaciÃ³n de 5 destinatarios en modo desarrollo, pero para uso personal (vos mismo) es suficiente.

---

## 8. Seguridad

- **ValidaciÃ³n de firma**: verificar `X-Hub-Signature-256` en cada webhook (evita requests falsos)
- **Whitelist**: solo procesar mensajes de tu nÃºmero
- **Variables de entorno**: tokens y secrets en `.env`, nunca hardcodeados
- **ngrok**: el tÃºnel usa HTTPS automÃ¡ticamente
- **SQLite**: opcionalmente cifrar con SQLCipher
- **Rate limiting**: mÃ¡ximo N mensajes por minuto (Fase 3)
- **No logging de tokens**: cuidado con los logs
- **Skills sandboxing**: los skills no pueden ejecutar cÃ³digo arbitrario, solo tools pre-definidas (Fase 4)

---

## 9. Requisitos del Sistema

- **Docker** y **Docker Compose**
- **ngrok** instalado (CLI) o como container
- **RAM**: 8GB mÃ­nimo (16GB recomendado)
- **GPU**: opcional pero 5-20x mÃ¡s rÃ¡pido (pass-through con `nvidia-container-toolkit` si usÃ¡s NVIDIA)
- **Disco**: ~5GB por modelo 7B + ~500MB imagen Docker + SQLite negligible
- **Internet**: necesaria (WhatsApp Cloud API + ngrok)
- **Cuenta Meta Developer** (gratis)

---

## 10. MÃ©tricas de Ã‰xito

- Responde en <10s con GPU, <30s sin GPU
- No pierde mensajes (dedup por `wa_message_id`)
- Mantiene conversaciÃ³n coherente de 20+ mensajes
- Memorias persisten entre reinicios
- Se levanta con `docker compose up`
- Cero costos de operaciÃ³n mensuales
- Sin bans ni warnings de WhatsApp

---

## 11. Riesgos y Mitigaciones

| Riesgo | Prob. | Impacto | MitigaciÃ³n |
|---|---|---|---|
| ngrok free tiene downtime | Baja | Medio | Los mensajes quedan en cola de Meta, llegan al reconectar |
| Calidad LLM local | Media | Medio | System prompt bien diseÃ±ado, probar varios modelos |
| Latencia sin GPU | Alta | Medio | Modelos mÃ¡s chicos (3B), o invertir en GPU |
| Context window overflow | Media | Bajo | Resumen automÃ¡tico, truncar historial (implementado Fase 2) |
| Meta cambia pricing de Cloud API | Baja | Bajo | Monitorear, los mensajes de servicio son gratis desde 2023 |
| ngrok cambia free tier | Baja | Medio | Alternativas: cloudflare tunnel, localhost.run |
| Tool calling unreliable en modelos chicos | Media | Medio | Fallback a regex parsing, limitar tools por modelo |

---

## 12. Fuera de Alcance (No-Goals)

- Grupos de WhatsApp (solo chat 1:1)
- Interfaz web de admin
- Multi-usuario (es un asistente personal single-user)
- Fine-tuning de modelos
- Deploy en cloud/VPS
- Mensajes proactivos (el bot no inicia conversaciÃ³n, solo responde)
- Multi-canal (solo WhatsApp â€” a diferencia de OpenClaw que soporta Telegram, Slack, etc.)

---

## 13. Estrategia Docker

### Servicios en `docker-compose.yml`

| Servicio | Imagen | PropÃ³sito |
|---|---|---|
| `wasap` | Build local (Dockerfile) | App FastAPI |
| `ollama` | `ollama/ollama` | LLM server |
| `ngrok` | `ngrok/ngrok` (opcional) | TÃºnel al webhook |

### Volumes persistentes
- `ollama_data` â†’ modelos descargados (no re-descargar en cada restart)
- `./data` â†’ SQLite DB + MEMORY.md + documentos

### GPU pass-through
- Para NVIDIA: `nvidia-container-toolkit` + `deploy.resources.reservations.devices` en compose
- Sin GPU: Ollama corre en CPU automÃ¡ticamente, no requiere config extra

### Workflow
```bash
# Primer uso
cp .env.example .env        # Configurar tokens
docker compose up -d         # Levanta todo
docker compose exec ollama ollama pull qwen3:8b   # Descargar modelo

# Uso diario
docker compose up -d         # Listo

# Ver logs
docker compose logs -f wasap
```

---

## 14. ComparaciÃ³n con OpenClaw

WasAP se inspira en varios patrones de OpenClaw pero con un enfoque mÃ¡s simple:

| Aspecto | OpenClaw | WasAP |
|---------|----------|-------|
| **Canales** | 12+ (WhatsApp, Telegram, Slack, Discord...) | Solo WhatsApp |
| **Usuarios** | Multi-agente, multi-usuario | Single-user |
| **LLM** | Cloud (Claude, GPT) + local | Solo local (Ollama) |
| **Skills** | 3000+ en ClawHub, carga en 3 tiers | Directorio local, carga progresiva |
| **Memoria** | MEMORY.md + daily notes + semantic search | MEMORY.md mirror + SQLite + semantic search (Fase 5) |
| **Identidad** | SOUL.md + AGENTS.md + USER.md | System prompt en .env |
| **Hosting** | Gateway WS local | FastAPI + ngrok |
| **Costo** | API keys de LLM cloud | Cero (todo local) |
| **Setup** | CLI install + gateway | `docker compose up` |

**Lo que tomamos de OpenClaw:**
- SKILL.md como definiciÃ³n declarativa de capacidades
- MEMORY.md como fuente de verdad human-readable
- Carga progresiva de skills (solo metadata al inicio, detalle on-demand)
- Memoria en dos capas (curada + diaria)
- BÃºsqueda hÃ­brida (vector + full-text)

**Lo que NO tomamos:**
- Gateway WebSocket (innecesario para single-channel)
- Multi-agente (somos single-user)
- Marketplace de skills (comunidad, ClawHub)
- Node system para dispositivos (cÃ¡mara, pantalla, etc.)
